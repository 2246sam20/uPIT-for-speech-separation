## Speech Separation with uPIT

Implement of "Multitalker Speech Separation With Utterance-Level Permutation Invariant Training of Deep Recurrent Neural Networks"

### requirements
see [requirements.txt](requirements.txt)

### Usage

1. training
    ```shell
    usage: run_pit.py [-h] [--config CONFIG] [--debug] [--num-epoches NUM_EPOCHES]

    Command to start PIT training, configured by .yaml files

    optional arguments:
    -h, --help            show this help message and exit
    --config CONFIG       Location of .yaml configure files for training
    --debug               If true, start training in debug data
    --num-epoches NUM_EPOCHES
                            Number of epoches to train
    ```

2. inference
    ```
    usage: separate.py [-h] [--cuda] [--dump-dir DUMP_DIR] [--dump-mask]
                    config state_dict wave_scp

    Command to seperate single-channel speech using masks generated by neural
    networks

    positional arguments:
    config               Location of training configure files
    state_dict           Location of networks state file
    wave_scp             Location of input wave scripts in kaldi format

    optional arguments:
    -h, --help           show this help message and exit
    --cuda               If true, inference on GPUs
    --dump-dir DUMP_DIR  Location to dump seperated speakers
    --dump-mask          If true, dump mask matrix
    ```
